{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xSSXdyCUvTHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "KARc4aZ_vqRN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywjL8-B6t2d1",
        "outputId": "538da9c8-5b11-481a-c5e2-184a625408f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'greenai'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 122 (delta 15), reused 1 (delta 1), pack-reused 100 (from 1)\u001b[K\n",
            "Receiving objects: 100% (122/122), 3.61 MiB | 21.64 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        " #\n",
        " !git clone https://github.com/indrani-16/greenai.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Set extraction directory\n",
        "extract_dir = \"5M_trees\"\n",
        "\n",
        "# Check if directory exists\n",
        "if not os.path.exists(extract_dir):\n",
        "    raise FileNotFoundError(f\"Directory {extract_dir} does not exist. Please check the path.\")\n",
        "\n",
        "# Define which columns to keep\n",
        "selected_columns = [\n",
        "    'common_name', 'scientific_name', 'city', 'state',\n",
        "    'longitude_coordinate', 'latitude_coordinate', 'address', 'condition',\n",
        "    'native', 'height_binned_M', 'diameter_breast_height_binned_CM',\n",
        "    'location_type', 'zipcode', 'neighborhood', 'location_name', 'ward',\n",
        "    'district', 'overhead_utility', 'diameter_breast_height_CM', 'height_M'\n",
        "]\n",
        "\n",
        "# Merge all CSVs except metadata\n",
        "exclude_files = {'Column_Headers_Dryad.csv', 'README_Dryad.txt'}\n",
        "csv_files = [f for f in glob.glob(os.path.join(extract_dir, \"*.csv\")) if os.path.basename(f) not in exclude_files]\n",
        "\n",
        "# Debug: Print found files\n",
        "print(f\"Found CSV files: {csv_files}\")\n",
        "\n",
        "df_list = []\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file, low_memory=False)\n",
        "\n",
        "    # Intersect the file's columns with selected_columns\n",
        "    available_cols = list(set(df.columns) & set(selected_columns))\n",
        "\n",
        "    # Add any missing columns with NaN to maintain uniformity\n",
        "    for col in selected_columns:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.NA\n",
        "\n",
        "    filtered_df = df[selected_columns].copy()\n",
        "    df_list.append(filtered_df)\n",
        "\n",
        "# Check if df_list is empty before concatenation\n",
        "if not df_list:\n",
        "    raise ValueError(\"No valid CSV files found to concatenate. Please check the directory or file names.\")\n",
        "\n",
        "# Merge all filtered data\n",
        "merged_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Add a unique tree ID\n",
        "merged_df.insert(0, 'tree_id', ['tree_' + str(i) for i in range(1, len(merged_df) + 1)])\n",
        "\n",
        "# Check for missing values\n",
        "print(merged_df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEyYUTEcwJ7C",
        "outputId": "f8a7013d-4107-401b-e635-5743396d6646"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found CSV files: ['/content/drive/MyDrive/5M_trees/SanFrancisco_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Ontario_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Sacramento_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/SantaRosa_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/CapeCoral_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/RanchoCucamonga_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/NewOrleans_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/SiouxFalls_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Irvine_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Jerseycity_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Louisville_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Fresno_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Minneapolis_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Seattle_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Worcester_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/GardenGrove_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Milwaukee_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Dallas_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/WashingtonDC_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Madison_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Baltimore_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/DesMoines_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Durham_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/LosAngeles_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Houston_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Nashville_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/OverlandPark_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Albuquerque_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Anaheim_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Stockton_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Denver_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Providence_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Phoenix_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Austin_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/StLouis_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/AuroraCO_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/ColoradoSprings_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Pittsburgh_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/GrandRapids_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Columbus_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Honolulu_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Detroit_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/OklahomaCity_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Orlando_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/HuntingtonBeach_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/LasVegas_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Plano_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Portland_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/SanJose_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Richmond_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Indianapolis_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Boston_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Miami_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Knoxville_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Rochester_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Greensboro_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Tampa_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/SanDiego_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Atlanta_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Oakland_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/NewYork_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Buffalo_Final_2022-06-18.csv', '/content/drive/MyDrive/5M_trees/Arlington_Final_2022-06-18.csv']\n",
            "tree_id                               0\n",
            "common_name                         126\n",
            "scientific_name                     126\n",
            "city                                126\n",
            "state                               126\n",
            "longitude_coordinate                126\n",
            "latitude_coordinate                 126\n",
            "address                             126\n",
            "condition                           126\n",
            "native                              126\n",
            "height_binned_M                     126\n",
            "diameter_breast_height_binned_CM    126\n",
            "location_type                       126\n",
            "zipcode                             126\n",
            "neighborhood                        126\n",
            "location_name                       126\n",
            "ward                                126\n",
            "district                            126\n",
            "overhead_utility                    126\n",
            "diameter_breast_height_CM           126\n",
            "height_M                            126\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with more than 3,038,500 missing values\n",
        "threshold = 3038501\n",
        "merged_df = merged_df.loc[:, merged_df.isnull().sum() <= threshold]"
      ],
      "metadata": {
        "id": "pOy09Utgz0LY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(columns=['diameter_breast_height_binned_CM'])\n"
      ],
      "metadata": {
        "id": "UKfzUPLrz1ta"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.dropna(subset=[\n",
        "    'common_name',\n",
        "    'scientific_name',\n",
        "    'longitude_coordinate',\n",
        "    'latitude_coordinate',\n",
        "    'condition',\n",
        "    'diameter_breast_height_CM','address', 'city'\n",
        "])"
      ],
      "metadata": {
        "id": "5vzOf8GPz4Y8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "6UY1vktxz_xm",
        "outputId": "3257c945-92f0-4a10-9ab1-24bd8ad5f7e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tree_id                      0\n",
              "common_name                  0\n",
              "scientific_name              0\n",
              "city                         0\n",
              "state                        0\n",
              "longitude_coordinate         0\n",
              "latitude_coordinate          0\n",
              "address                      0\n",
              "condition                    0\n",
              "native                       0\n",
              "height_binned_M              0\n",
              "location_type                0\n",
              "zipcode                      0\n",
              "neighborhood                 0\n",
              "location_name                0\n",
              "ward                         0\n",
              "district                     0\n",
              "overhead_utility             0\n",
              "diameter_breast_height_CM    0\n",
              "height_M                     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tree_id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>common_name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>scientific_name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>city</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>state</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>longitude_coordinate</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>latitude_coordinate</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>address</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>condition</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>native</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>height_binned_M</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>location_type</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zipcode</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neighborhood</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>location_name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ward</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>district</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overhead_utility</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diameter_breast_height_CM</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>height_M</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "merged_df = pd.DataFrame({\n",
        "    'common_name': ['Oak', 'Maple', 'Oak', 'Pine', 'Maple', ' ', None, 'Birch', 'Birch']\n",
        "})\n",
        "  # make sure this file exists in your working directory\n"
      ],
      "metadata": {
        "id": "EK0hFB-K4fFf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Step 1: Count species and filter those with at least 2 occurrences\n",
        "species_counts = merged_df['common_name'].value_counts()\n",
        "valid_species = species_counts[species_counts >= 2].index.tolist()\n",
        "\n",
        "# Step 2: Filter valid and non-null species\n",
        "filtered_df = merged_df[\n",
        "    merged_df['common_name'].isin(valid_species) &\n",
        "    merged_df['common_name'].notna()\n",
        "].copy()\n",
        "\n",
        "# Step 3: Remove empty strings (after stripping whitespace)\n",
        "filtered_df = filtered_df[filtered_df['common_name'].str.strip() != ''].copy()\n",
        "\n",
        "# Step 4: Sanity check\n",
        "assert filtered_df['common_name'].value_counts().min() >= 2, \"Still has species with < 2 samples!\"\n",
        "\n",
        "# Final cleaned data\n",
        "data = filtered_df.copy()\n",
        "\n",
        "# Output value counts\n",
        "print(data['common_name'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRKHf2g719ZF",
        "outputId": "290e29e6-18ec-4ca0-8fd1-43c578ee750f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "common_name\n",
            "Oak      2\n",
            "Maple    2\n",
            "Birch    2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select your CSV file manually\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"your_uploaded_file.csv\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "df = pd.read_csv(\"5M_trees_cleaned.csv\")\n",
        "\n",
        "# Load the data (Replace with your actual filtered DataFrame or CSV)\n",
        "df = pd.read_csv(\"trees.csv\")  # or use `filtered_df.copy()` if already defined\n",
        "\n",
        "# Optional: Simplify scientific name to genus\n",
        "df['genus'] = df['scientific_name'].apply(lambda x: x.split()[0] if pd.notnull(x) else '')\n",
        "\n",
        "# Encode categorical variables (ensure no NaNs)\n",
        "df['native_encoded'] = df['native'].astype('category').cat.codes\n",
        "df['city_encoded'] = df['city'].astype('category').cat.codes\n",
        "df['state_encoded'] = df['state'].astype('category').cat.codes\n",
        "\n",
        "# Features to use for the model\n",
        "feature_cols = [\n",
        "    'latitude_coordinate',\n",
        "    'longitude_coordinate',\n",
        "    'diameter_breast_height_CM',\n",
        "    'native_encoded',\n",
        "    'city_encoded',\n",
        "    'state_encoded'\n",
        "]\n",
        "\n",
        "# Prepare feature matrix\n",
        "X = df[feature_cols]\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit the Nearest Neighbors model\n",
        "nn_model = NearestNeighbors(n_neighbors=50, algorithm='ball_tree')\n",
        "nn_model.fit(X_scaled)\n",
        "\n",
        "# Recommender function\n",
        "def recommend_species(lat, lon, diameter_cm, native, city, state, top_n=5):\n",
        "    # Encode the input values based on existing categories\n",
        "    try:\n",
        "        native_code = df['native'].astype('category').cat.categories.get_loc(native)\n",
        "        city_code = df['city'].astype('category').cat.categories.get_loc(city)\n",
        "        state_code = df['state'].astype('category').cat.categories.get_loc(state)\n",
        "    except KeyError as e:\n",
        "        return f\"Value not found in dataset categories: {e}\"\n",
        "\n",
        "    # Prepare and scale input\n",
        "    input_features = np.array([[lat, lon, diameter_cm, native_code, city_code, state_code]])\n",
        "    input_scaled = scaler.transform(input_features)\n",
        "\n",
        "    # Find nearest neighbors\n",
        "    distances, indices = nn_model.kneighbors(input_scaled)\n",
        "\n",
        "    # Count species among neighbors\n",
        "    neighbors = df.iloc[indices[0]]\n",
        "    species_counts = Counter(neighbors['common_name'])\n",
        "\n",
        "    # Return top N recommended species\n",
        "    return species_counts.most_common(top_n)\n",
        "\n",
        "# Example usage (modify values as needed)\n",
        "recommendations = recommend_species(\n",
        "    lat=37.77,\n",
        "    lon=-122.42,\n",
        "    diameter_cm=30,\n",
        "    native='Yes',\n",
        "    city='San Francisco',\n",
        "    state='California',\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "print(\"Recommended species:\", recommendations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "5ocGr1Zn2v5j",
        "outputId": "0ccf536a-25fb-4ab1-83b4-21b6f06db355"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e51019db-02cd-4293-9bfc-d5a16d6d47a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e51019db-02cd-4293-9bfc-d5a16d6d47a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_uploaded_file.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1540785925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"your_uploaded_file.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_uploaded_file.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "recommendation = recommend_species(\n",
        "    lat=38.2274,\n",
        "    lon=-85.8009,\n",
        "    diameter_cm=1.2,\n",
        "    native='naturally_occurring',\n",
        "    city='Louisville',\n",
        "    state='Kentucky',\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "for species, count in recommendation:\n",
        "    print(f\"{species} (seen {count} times nearby)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7sA5Al7q2-Fn",
        "outputId": "2c3776e7-a606-416b-b865-d4c53578d886"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'recommend_species' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2377632157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m recommendation = recommend_species(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m38.2274\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m85.8009\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdiameter_cm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'recommend_species' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_recommender(X_scaled, df, model, top_k=5, sample_size=1000):\n",
        "    correct = 0\n",
        "    ranks = []\n",
        "\n",
        "    for i in tqdm(range(sample_size)):\n",
        "        x_query = X_scaled[i].reshape(1, -1)\n",
        "        distances, indices = model.kneighbors(x_query)\n",
        "\n",
        "        # exclude itself\n",
        "        neighbor_indices = [idx for idx in indices[0] if idx != i][:top_k]\n",
        "        true_species = df.iloc[i]['common_name']\n",
        "        neighbor_species = df.iloc[neighbor_indices]['common_name'].tolist()\n",
        "\n",
        "        if true_species in neighbor_species:\n",
        "            correct += 1\n",
        "            ranks.append(neighbor_species.index(true_species) + 1)\n",
        "        else:\n",
        "            ranks.append(0)\n",
        "\n",
        "    hit_rate = correct / sample_size\n",
        "    mean_rank = sum([1/r for r in ranks if r > 0]) / sample_size\n",
        "\n",
        "    print(f\"Top-{top_k} Hit Rate: {hit_rate:.4f}\")\n",
        "    print(f\"Mean Reciprocal Rank: {mean_rank:.4f}\")\n",
        "    return hit_rate, mean_rank\n",
        "\n",
        "# Run evaluation on a 1000-sample subset\n",
        "evaluate_recommender(X_scaled, df, nn_model, top_k=5, sample_size=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "o8KEidaG304A",
        "outputId": "e192d61a-7e5f-477f-90e2-3115d5132d59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_scaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3444545221.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Run evaluation on a 1000-sample subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mevaluate_recommender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_scaled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save scaler and model\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "joblib.dump(nn_model, 'nn_model.joblib')\n",
        "\n",
        "# Also save the dataframe with encoded columns (needed for categories and lookup)\n",
        "df.to_pickle('tree_data.pkl')\n",
        "\n",
        "print(\"Saved scaler, model and data!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "dQmO9rJT75et",
        "outputId": "a2f7ca70-9e98-4f15-b5b8-a853c65ff21a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scaler' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4038283450.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save scaler and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scaler.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nn_model.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scaler' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_common_locations_for_species(tree_name, top_n=10):\n",
        "    \"\"\"\n",
        "    Given a tree common name, return the top N most frequent locations.\n",
        "    \"\"\"\n",
        "    species_df = df[df['common_name'] == tree_name]\n",
        "\n",
        "    if species_df.empty:\n",
        "        return f\"No records found for species: {tree_name}\"\n",
        "\n",
        "    # You can group by city/state or full address\n",
        "    location_counts = species_df.groupby(['city', 'state']) \\\n",
        "                                .size().reset_index(name='count') \\\n",
        "                                .sort_values(by='count', ascending=False) \\\n",
        "                                .head(top_n)\n",
        "\n",
        "    return location_counts"
      ],
      "metadata": {
        "id": "U3qs1FBU7_7h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tree name\n",
        "tree_name = 'Bur oak'\n",
        "top_locations = get_common_locations_for_species(tree_name, top_n=5)\n",
        "\n",
        "print(f\"Top locations where '{tree_name}' is commonly found:\")\n",
        "print(top_locations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mLxJaos8EsN",
        "outputId": "6a52d3ea-b39e-424d-c5dc-9ccc8e759819"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top locations where 'Bur oak' is commonly found:\n",
            "No records found for species: Bur oak\n"
          ]
        }
      ]
    }
  ]
}